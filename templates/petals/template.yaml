apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_id}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_id}}
spec:
  queue: {{queue_name}}
  minAvailable: 1
  schedulerName: volcano
  priorityClassName: {{priority}}
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  policies: 
  - event: PodEvicted # Restart the job when a pod is evicted.
    action: RestartJob
  tasks:
  - replicas: 1   # One ps pod specified
    name: sidecar
    template: # Definition of the ps pod
      metadata:
        labels:
          role: leader
          kalavai.job.name: {{deployment_id}}
      spec:
        containers:
        - image: docker.io/bundenth/health.petals:latest
          name: petals-health
          command:
          - sh
          - -c
          - |
            # set initial peers on config.py
            if [ -z "{{initial_peers}}" ]
            then
              echo "Public peers"
            else
              echo "Private peers"
              cat _health_config.py | awk '{gsub(/INITIAL_PEERS=PUBLIC_INITIAL_PEERS/,"INITIAL_PEERS=[\"{{initial_peers}}\"]")}1' > config.py
            fi
            gunicorn app:app --bind 0.0.0.0:5000 --worker-class gthread --threads 1 --timeout 1000
          ports:
          - containerPort: 5000
            name: health-port
          resources:
            requests:
              cpu: 0.5
              memory: 1Gi
            limits:
              cpu: 1
              memory: 1Gi
        - image: docker.io/bundenth/chat.petals:latest
          name: petals-chat
          command:
          - sh
          - -c
          - |
            # set initial peers on config.py
            if [ -z "{{initial_peers}}" ]
            then
              echo "Public peers"
              cat _chat_config.py | awk '{gsub(/MODEL_ID/,"{{model_id}}")}1' > config.py
            else
              echo "Private peers"
              cat _chat_config.py | awk '{gsub(/INITIAL_PEERS=PUBLIC_INITIAL_PEERS/,"INITIAL_PEERS=[\"{{initial_peers}}\"]")}1' > _config.py
              cat _config.py | awk '{gsub(/MODEL_ID/,"{{model_id}}")}1' > config.py
            fi
            gunicorn app:app --bind 0.0.0.0:5001 --worker-class gthread --threads 1 --timeout 1000
          ports:
          - containerPort: 5001
            name: chat-port
          env:
          - name: HF_HOME
            value: /cache
          - name: HF_TOKEN
            value: {{hf_token}}
          resources:
            requests:
              cpu: 1
              memory: 4Gi
            limits:
              cpu: 4
              memory: 4Gi
          volumeMounts:
            - name: cache
              mountPath: /cache
        volumes:
        # host path, persistent in local node, but cannot limit capacity
        # - name: temp-volume
        #   emptyDir:
        #     medium: "" # default to disk; "Memory" for RAM usage
        #     sizeLimit: 10Gi
        - name: cache
          hostPath:
            path: {{mount_path}}/kalavai
            type: DirectoryOrCreate
        # - name: cache
        #   persistentVolumeClaim:
        #     claimName: {{storage}}
        restartPolicy: OnFailure
  - replicas: {{max_workers}}   # One ps pod specified
    name: worker
    template: # Definition of the ps pod
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        runtimeClassName: nvidia
        containers:
        - image: docker.io/learningathome/petals:main
          name: petals-worker
          command:
          - sh
          - -c
          - |
            if [ -z "{{initial_peers}}" ]
            then
              INITIAL_PEERS=""
            else
              INITIAL_PEERS="--initial_peers {{initial_peers}}"
            fi
            python -m petals.cli.run_server \
              {{model_id}} \
              --cache_dir /cache/hub \
              --public_name kalavai.net $INITIAL_PEERS \
              {{extra}}
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          - name: HF_HOME
            value: /cache
          resources:
            requests:
              cpu: {{min_cpus}}
              memory: {{min_memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: "{{ephemeral_storage}}Gi"
            limits:
              cpu: {{max_cpus}}
              memory: {{max_memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: "{{ephemeral_storage}}Gi"
          volumeMounts:
          - name: temp-volume
            mountPath: /cache
        volumes:
        # - name: temp-volume
        #   emptyDir:
        #     medium: "" # default to disk; "Memory" for RAM usage
        #     sizeLimit: 10Gi
        # host path, persistent in local node, but cannot limit capacity
        - name: temp-volume
          hostPath:
            path: {{mount_path}}/kalavai
            type: DirectoryOrCreate
        restartPolicy: OnFailure