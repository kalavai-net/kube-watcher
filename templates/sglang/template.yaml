apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_id}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_id}}
spec:
  queue: default
  schedulerName: volcano
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  maxRetries: {{max_retries}}
  tasks:
{% if litellm_key != "" %}
  - replicas: 1   # One ps pod specified
    name: registrar
    policies:
    - event: PodEvicted
      action: RestartJob
    - event: PodFailed
      action: RestartJob
    - event: TaskCompleted
      action: RestartJob
    - event: Unknown
      action: RestartJob
    template: # Definition of the ps pod
      terminationGracePeriodSeconds: 30 #give enough time to the preStop hook
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        {% if NODE_SELECTORS %}
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
            {% if NODE_SELECTORS_OPS == "OR" %}
              {% for selector in NODE_SELECTORS %}
              - matchExpressions:
                - key: {{selector.name}}
                  operator: In
                  values:
                  {% for vals in selector.value %}
                  - "{{vals}}"
                  {% endfor %}
              {% endfor %}
            {% else %}
              - matchExpressions:
              {% for selector in NODE_SELECTORS %}
                {% for vals in selector.value %}
                - key: {{selector.name}}
                  operator: In
                  values:
                  - "{{vals}}"
                {% endfor %}
              {% endfor %}
            {% endif %}
        {% endif %}
        containers:
        - name: registrar-leader
          image: docker.io/bundenth/kalavai-utils:latest
          command:
          - sh
          - -c
          - |
            echo "Waiting for model to be served..."
            PS_HOST=`cat /etc/volcano/server.host`;
            /workspace/wait_for_service.sh --servers="$PS_HOST" --port=8080
            export MODEL_API_BASE="http://"$PS_HOST"."$VC_NAMESPACE":8080/v1";
            /workspace/start_point.sh \
              --litellm_kalavai_extras='{{litellm_kalavai_extras}}' \
              --litellm_access_group={{litellm_access_group}}
          env:
          - name: VC_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: LITELLM_MODEL_NAME
          {% if model_name_override != "" %}
            value: "{{model_name_override}}"
          {% else %}
            value: "{{model_id}}"
          {% endif %}
          - name: LITELLM_BASE_URL
            value: {{litellm_base_url}}
          - name: LITELLM_KEY
            value: {{litellm_key}}
          - name: MODEL_ID
            value: {{model_id}}
          - name: DEPLOYMENT_ID
            value: {{deployment_id}}
          - name: PROVIDER
            value: hosted_vllm
          lifecycle:
            preStop:
              exec:
                command: 
                - sh
                - -c
                - |
                  /workspace/cleanup.sh
          resources:
            requests:
              cpu: 0.05
              memory: 0.05Gi
            limits:
              cpu: 0.05
              memory: 0.05Gi
        restartPolicy: OnFailure
{% endif %}
  - replicas: 1   # One ps pod specified
    name: server
    policies:
    - event: PodEvicted
      action: RestartJob
    - event: PodFailed
      action: RestartJob
    - event: TaskCompleted
      action: RestartJob
    - event: Unknown
      action: RestartJob
    template: # Definition of the ps pod
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          role: leader
          kalavai.job.name: {{deployment_id}}
      spec:
        runtimeClassName: nvidia
        {% if NODE_SELECTORS %}
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
            {% if NODE_SELECTORS_OPS == "OR" %}
              {% for selector in NODE_SELECTORS %}
              - matchExpressions:
                - key: {{selector.name}}
                  operator: In
                  values:
                  {% for vals in selector.value %}
                  - "{{vals}}"
                  {% endfor %}
              {% endfor %}
            {% else %}
              - matchExpressions:
              {% for selector in NODE_SELECTORS %}
                {% for vals in selector.value %}
                - key: {{selector.name}}
                  operator: In
                  values:
                  - "{{vals}}"
                {% endfor %}
              {% endfor %}
            {% endif %}
        {% endif %}
        containers:
        - name: sglang-leader
          image: docker.io/bundenth/ray-sglang-cuda:latest
          command:
          - sh
          - -c
          - |
            PS_HOST=`cat /etc/volcano/server.host`;
            # Download model weights
            # /home/ray/workspace/download_model.sh \
            #   --model_id={{model_id}} \
            #   --remote_dir="/home/ray/cache";
            nvidia-smi;
            # Run model
            # --model_path="/home/ray/cache/{{model_id}}" \
            /home/ray/workspace/run_model.sh \
              --command="server" \
              --model_path="{{model_id}}" \
              --model_id={{model_id}} \
              --server_ip=$PS_HOST:5000 \
              --extra='{{extra}}' \
              --template_url='{{template_url}}' \
              --tensor_parallel_size={{gpus}} \
              --pipeline_parallel_size={{workers}} \
              --tool_call_parser={{tool_call_parser}} \
              --node_rank=0 \
              --num_nodes={{workers}};
            exit 1
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          ports:
          - containerPort: 8080
            name: model-port
          # this blocks internal traffic, which causes issues when tasks wait for each other
          # readinessProbe:
          #   httpGet:
          #     path: /health
          #     port: 8080
          #   initialDelaySeconds: 5
          #   periodSeconds: 10
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: {{working_memory}}Gi
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              nvidia.com/gpumem-percentage: {{cuda_gpu_mem_percentage}}
              ephemeral-storage: {{working_memory}}Gi
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: {{memory * 0.5}}Gi
        restartPolicy: OnFailure
  - replicas: {{workers - 1}}
    name: worker
    policies:
    - event: PodEvicted
      action: RestartJob
    - event: PodFailed
      action: RestartJob
    - event: TaskCompleted
      action: RestartJob
    - event: Unknown
      action: RestartJob
    template: # Definition of worker pods
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        runtimeClassName: nvidia
        {% if NODE_SELECTORS %}
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
            {% if NODE_SELECTORS_OPS == "OR" %}
              {% for selector in NODE_SELECTORS %}
              - matchExpressions:
                - key: {{selector.name}}
                  operator: In
                  values:
                  {% for vals in selector.value %}
                  - "{{vals}}"
                  {% endfor %}
              {% endfor %}
            {% else %}
              - matchExpressions:
              {% for selector in NODE_SELECTORS %}
                {% for vals in selector.value %}
                - key: {{selector.name}}
                  operator: In
                  values:
                  - "{{vals}}"
                {% endfor %}
              {% endfor %}
            {% endif %}
        {% endif %}
        containers:
        - name: sglang-worker
          image: docker.io/bundenth/ray-sglang-cuda:latest
          command:
          - sh
          - -c
          - |
            PS_HOST=`cat /etc/volcano/server.host`;
            # Download model weights
            # /home/ray/workspace/download_model.sh \
            #   --model_id={{model_id}} \
            #   --remote_dir="/home/ray/cache";
            nvidia-smi;
            # Run model
            RANK=$(($VC_TASK_INDEX + 1));
            echo "Node rank: "$RANK;
            # --model_path="/home/ray/cache/{{model_id}}" \
            /home/ray/workspace/run_model.sh \
              --command="worker" \
              --model_path="{{model_id}}" \
              --model_id={{model_id}} \
              --server_ip=$PS_HOST:5000 \
              --extra='{{extra}}' \
              --template_url='{{template_url}}' \
              --tensor_parallel_size={{gpus}} \
              --pipeline_parallel_size={{workers}} \
              --tool_call_parser={{tool_call_parser}} \
              --node_rank=$RANK \
              --dist-timeout={{dist_timeout}} \
              --num_nodes={{workers}};
            exit 1
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}              
              ephemeral-storage: {{working_memory}}Gi
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              nvidia.com/gpumem-percentage: {{cuda_gpu_mem_percentage}}
              ephemeral-storage: {{working_memory}}Gi
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: {{memory * 0.5}}Gi
        restartPolicy: OnFailure
---
# deploy NodePort to expose model (if not using LiteLLM registration)
{% if litellm_key == "" %}
apiVersion: v1
kind: Service
metadata:
  name: {{deployment_id}}-service
  labels:
    # must have this!
    kalavai.job.name: {{deployment_id}}
spec:
  type: NodePort
  selector:
    role: leader
    kalavai.job.name: {{deployment_id}}
  ports:
    - name: main
      port: 8080
      targetPort: 8080
      protocol: TCP
    {% if nodeport != "" %}
      nodePort: {{nodeport}}
    {% endif %}
{% endif %}