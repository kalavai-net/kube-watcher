{
    "name": "SGLang",
    "type": "model",
    "description": "SGLang is a fast serving framework for large language models and vision language models.",
    "docs": "https://docs.sglang.ai/",
    "icon": "https://docs.sglang.ai/_static/logo.png",
    "template_rules": "SGLang is an LLM model serving template on GPUs. With the SGLang template you can deploy a SGLang server to serve your LLM models in your cluster. SGLang does not support GGUF models. Supported models: \n\nLarge language models: DeepSeek (v1, v2, v3/R1), Qwen (3, 3MoE, 2.5, 2 series), Llama (2, 3.x, 4 series), Mistral (Mixtral, NeMo, Small3), Gemma (v1, v2, v3), Phi (Phi-3, Phi-4 series), MiniCPM (v3, 4B), OLMoE (Open MoE), StableLM (3B, 7B), Command-R (Cohere), DBRX (Databricks), Grok (xAI), ChatGLM (GLM-130B family), InternLM 2 (7B, 20B), ExaONE 3 (Korean-English), Baichuan 2 (7B, 13B), XVERSE (MoE), SmolLM (135Mâ€“1.7B), GLM-4 (Multilingual 9B), MiMo (7B series); Multimodal models: Qwen-VL (Qwen2 series), DeepSeek-VL2, Janus-Pro (1B, 7B), MiniCPM-V / MiniCPM-o, Llama 3.2 Vision (11B), LLaVA (v1.5 & v1.6), LLaVA-NeXT (8B, 72B), LLaVA-OneVision, Gemma 3 (Multimodal), Kimi-VL (A3B), Mistral-Small-3.1-24B, Phi-4-multimodal-instruct; Embedding models: Llama/Mistral based (E5EmbeddingModel), GTE (QwenEmbeddingModel), GME (MultimodalEmbedModel), CLIP (CLIPEmbeddingModel), BGE (BgeEmbeddingModel); Reward models: Llama (3.1 Reward / LlamaForSequenceClassification), Gemma 2 (27B Reward / Gemma2ForSequenceClassification), InternLM 2 (Reward / InternLM2ForRewardMode), Qwen2.5 (Reward - Math / Qwen2ForRewardModel), Qwen2.5 (Reward - Sequence / Qwen2ForSequenceClassification).",
    "values_rules": "'model_id' corresponds to the huggingface model id.\n||\n'cpus' refers to the number of cpus to use per worker, and its value should always be less than the number of freely available CPUs in each node on the pool.\n||\n'gpus' refers to the number of GPUs to use on any single node used in the deployment; generally this value should be 1 unless at least as many nodes as 'workers' have multiple GPUs.\n||\n'working_memory' sets the disk memory set aside for each worker node, and should be enough to accommodate the size of the model; a good heuristic is to leave 2 times the model size (in billions of parameters), in GBs (e.g. if the model is 3B parameters, set the working memory to 6 GBs)\n||\n'workers' sets the number of nodes or devices to use in the deployment, and should always be less or equal than the number of nodes with GPUs."
}

