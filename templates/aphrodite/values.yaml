############################
### MUST HAVE THESE FIELDS ##
- name: id_field
  value: model_id
  default: model_id
  editable: false
  required: false
  description: "Field that contains the ID of the job"
############################

- name: litellm_base_url
  value: "http://litellm.default.svc.cluster.local:4000" #"http://192.168.68.67:30219"
  default: "http://litellm.default.svc.cluster.local:4000"
  editable: true
  required: false
  description: "Base URL of the LiteLLM service (central model registry). This can be an external LiteLLM instance or an internal deployment in the pool (default)"

- name: litellm_key
  value: ""
  default: ""
  editable: true
  required: false
  description: "Master key of the LiteLLM service (central model registry)"

- name: working_memory
  value: 10
  default: 10
  editable: true
  required: true
  description: "Temporary storage to use to cache model weights (in GB), should be big enough to hold the model weights. All workers should have enough free disk to accommodate the weights."

- name: workers
  value: 1
  default: 1
  editable: true
  required: true
  description: "Number of workers, corresponding to the number of nodes (or machines) that will be used to deploy the model"

- name: model_id
  value: null
  default: null
  editable: true
  required: true
  description: "Huggingface model id to load"

- name: hf_token
  value: <your token>
  default: null
  editable: true
  required: true
  description: "Huggingface token, only required to load model weights"

- name: cpus
  value: 2
  default: 2
  editable: true
  required: false
  description: "CPUs to be used per single worker (final one = cpus * workers). Workers should have these many CPUs available."

- name: gpus
  value: 1
  default: 1
  editable: true
  required: false
  description: "GPUs to be used per single worker (final one = gpus * workers). Workers should have these many GPUs available."

- name: memory
  value: 8
  default: 8
  editable: true
  required: false
  description: "RAM memory to be used per single worker (final one = memory * workers). Workers should have these much RAM available."

- name: tensor_parallel_size
  value: "1"
  default: "1"
  editable: true
  required: false
  description: "Tensor parallelism (use the number of GPUs per node)"

- name: pipeline_parallel_size
  value: "1"
  default: "1"
  editable: true
  required: false
  description: "Pipeline parallelism (use the number of nodes)"

- name: tool_call_parser
  value: "llama3_json"
  default: "llama3_json"
  editable: true
  required: false
  description: "Tool call parser to use. https://aphrodite.pygmalion.chat/usage/openai/#command-line-arguments-for-the-server"

- name: extra
  value: ""
  default: ""
  editable: true
  required: false
  description: "Extra parameters to pass to the vLLM server. See https://aphrodite.pygmalion.chat/usage/openai/#command-line-arguments-for-the-server"

- name: max_retries
  value: 1000
  default: 1000
  editable: true
  required: false
  description: "If the job fails, how many times to retry redeployment"