apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_id}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_id}}
spec:
  queue: default
  schedulerName: volcano
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  maxRetries: {{max_retries}}
  tasks:
{% if litellm_key != "" %}
  - replicas: 1   # One ps pod specified
    name: registrar
    policies:
    - event: PodEvicted
      action: RestartTask
    - event: PodFailed
      action: RestartTask
    - event: TaskCompleted
      action: RestartTask
    - event: Unknown
      action: RestartTask
    template: # Definition of the ps pod
      terminationGracePeriodSeconds: 30 #give enough time to the preStop hook
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
      {% if NODE_SELECTORS %}
        nodeSelector:
        {% for selector in NODE_SELECTORS %}
          {{selector.name}}: {{selector.value}}
        {% endfor %}
      {% endif %}
        containers:
        - name: llamacpp-registrar
          image: docker.io/bundenth/kalavai-utils:latest
          env:
          - name: VC_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          command:
          - sh
          - -c
          - |
            # wait for model to be served
            echo "Waiting for model service..."
            PS_HOST=`cat /etc/volcano/server.host`;
            /workspace/wait_for_service.sh --servers="$PS_HOST" --port=8080
            # Register model with LiteLLM
            PS_HOST=`cat /etc/volcano/server.host`;
            LITELLM_MODEL_NAME="{{model_filename}}";
            echo "Creating new entry on LiteLLM: (host: $PS_HOST) - (model id: $LITELLM_MODEL_NAME)";
            API_BASE="http://"$PS_HOST"."$VC_NAMESPACE":8080/v1";
            /workspace/register_model.sh \
              --litellm_base_url={{litellm_base_url}} \
              --litellm_key={{litellm_key}} \
              --litellm_model_name="$LITELLM_MODEL_NAME" \
              --model_id={{model_filename}} \
              --provider=openai \
              --api_base=$API_BASE \
              --job_id={{deployment_id}} \
              --model_info='{"extra": "llamacpp parameters: {{server_extra}}", "cpus": "{{cpus}}", "gpus": "{{gpus}}", "memory": "{{memory}}", "cpu_workers": "{{cpu_workers}}", "gpu_workers": "{{gpu_workers}}"}'
          lifecycle:
            preStop:
              exec:
                command: 
                - sh
                - -c
                - |
                  LITELLM_MODEL_NAME="{{model_filename}}";         
                  MODEL_ID=$(python3 /workspace/get_litellm_id.py \
                    --litellm_url={{litellm_base_url}} \
                    --api_key={{litellm_key}} \
                    --job_id={{deployment_id}} \
                    --model_name="$LITELLM_MODEL_NAME");
                  curl -X POST "{{litellm_base_url}}/model/delete" \
                      -H 'Authorization: Bearer {{litellm_key}}' \
                      -H "accept: application/json" \
                      -H "Content-Type: application/json" \
                      -d '{ "id": "'$MODEL_ID'"}';
          resources:
            requests:
              cpu: 0.5
              memory: 0.5Gi
            limits:
              cpu: 0.5
              memory: 0.5Gi
        restartPolicy: OnFailure
{% endif %}
  - replicas: 1   # One ps pod specified
    name: server
    policies:
    - event: PodEvicted
      action: RestartJob
    - event: PodFailed
      action: RestartJob
    - event: TaskCompleted
      action: RestartJob
    - event: Unknown
      action: RestartJob
    template: # Definition of the ps pod
      metadata:
{% if gpus > 0 %}
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
{% endif %}
        labels:
          role: leader
          kalavai.job.name: {{deployment_id}}
      spec:
{% if gpus > 0 %}
        runtimeClassName: nvidia
{% endif %}
      {% if NODE_SELECTORS %}
        nodeSelector:
        {% for selector in NODE_SELECTORS %}
          {{selector.name}}: {{selector.value}}
        {% endfor %}
      {% endif %}
        containers:
        - name: llamacpp-leader
{% if gpus > 0 %}
          image: docker.io/bundenth/llamacpp-gpu:latest
{% else %}
          image: docker.io/bundenth/llamacpp-cpu:latest
{% endif %}
          command:
          - sh
          - -c
          - |
{% if gpus > 0 %}
            /workspace/build.sh server_gpu;
{% else %}
            /workspace/build.sh server_cpu;
{% endif %}
            HOSTS=`cat /etc/volcano/worker.host`;
            export WORKERS=$(/workspace/get_workers_address.sh --rpc_port={{rpc_port}} --rpc_servers="${HOSTS}")
            /workspace/run_api_server.sh \
              --repo_id={{repo_id}} \
              --model_filename={{model_filename}} \
              --local_dir=/cache \
              --port=8080 \
              --rpc_servers=$WORKERS \
              --parallel={{parallel}} \
              --extra='{{server_extra}}'
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          ports:
          - containerPort: 8080
            name: model-port
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              ephemeral-storage: {{working_memory}}Gi
{% if gpus > 0 %}
              nvidia.com/gpu: {{gpus}}
{% endif %}
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              ephemeral-storage: {{working_memory}}Gi
{% if gpus > 0 %}
              nvidia.com/gpu: {{gpus}}
{% endif %}
          volumeMounts:
            - name: cache
              mountPath: /cache
        volumes:
        - name: cache
          emptyDir: {}
        restartPolicy: OnFailure
  - replicas: {{workers - 1}}
    name: worker
    policies:
    - event: PodEvicted
      action: RestartJob
    - event: PodFailed
      action: RestartJob
    - event: TaskCompleted
      action: RestartJob
    - event: Unknown
      action: RestartJob
    template: # Definition of worker pods
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
{% if gpus > 0 %}
        runtimeClassName: nvidia
{% endif %}
      {% if NODE_SELECTORS %}
        nodeSelector:
        {% for selector in NODE_SELECTORS %}
          {{selector.name}}: {{selector.value}}
        {% endfor %}
      {% endif %}
        containers:
        - name: llamacpp-worker
{% if gpus > 0 %}
          image: docker.io/bundenth/llamacpp-gpu:latest
{% else %}
          image: docker.io/bundenth/llamacpp-cpu:latest
{% endif %}
          command:
          - sh
          - -c
          - |
{% if gpus > 0 %}
            /workspace/build.sh gpu;
{% else %}
            /workspace/build.sh cpu;
{% endif %}
            /workspace/run_rpc_worker.sh --rpc_port={{rpc_port}}
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
{% if gpus > 0 %}
              nvidia.com/gpu: {{gpus}}
{% endif %}
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
{% if gpus > 0 %}
              nvidia.com/gpu: {{gpus}}
{% endif %}
        restartPolicy: OnFailure
