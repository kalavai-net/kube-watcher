{
    "name": "llama.cpp",
    "type": "model",
    "description": "LLM inference in C/C++",
    "docs": "https://github.com/ggml-org/llama.cpp",
    "icon": "https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png",
    "template_rules": "llamacpp is a model serving template specifically designed for GGUF LLMs. With the llama.cpp template you can deploy llama.cpp models on your cluster. Compatible models include those with the GGUF format. Perfect for CPU only inference as it does not require GPUs to be available.",
    "values_rules": "To select a model, one needs to set both a 'repo_id' and a 'model_filename'. 'repo_id' corresponds to the huggingface model id. 'model_filename' is the filename, within the huggingface repository, that we want to deploy (normally this specifies the quantisation level).\n||\n'cpus' refers to the number of cpus to use per worker, and its value should always be less than the number of freely available CPUs in each node on the pool; usually a value of 2 suffices.\n||\n'gpus' refers to the number of GPUs to use on any single node used in the deployment; this should normally be 0 for a CPU only deployment, but can be 1 if there are enough nodes with GPUs available.\n||\n'working_memory' sets the disk memory set aside for each worker node, and should be enough to accommodate the size of the model; a good heuristic is to leave 2 times the model size (in billions of parameters), in GBs (e.g. if the model is 3B parameters, set the working memory to 6 GBs)\n||\n'workers' sets the number of nodes or devices to use in the deployment, and should always be less or equal than the number of nodes with GPUs."
}