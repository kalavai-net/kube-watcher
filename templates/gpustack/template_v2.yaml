apiVersion: v1
stringData:
  hf_token: {{hf_token}}
kind: Secret
metadata:
  name: {{deployment_id}}-secret
  labels:
    kalavai.job.name: {{deployment_id}}
type: Opaque
immutable: true
---
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: gpustack-config
# data:
#   server.yaml: |
#     # Common Options
#     debug: true
#     #data_dir: /path/to/dir
#     token: {{token}}

#     # Server Options
#     host: 0.0.0.0
#     port: 80
#     disable_worker: true
#     force_auth_localhost: false
#     bootstrap_password: {{admin_password}}
#     system_reserved:
#       ram: 2
#       vram: 2
    
#     huggingface_token: {{hf_token}}
---
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_id}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_id}}
spec:
  minAvailable: 1
  queue: default
  schedulerName: volcano
  priorityClassName: {{JOB_PRIORITY|default("user-spot-priority", true)}}
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  maxRetry: 100000
  tasks:
  - replicas: 1   # One ps pod specified
    name: server
    maxRetry: 100000
    policies:
    - event: "PodFailed"
      action: RestartJob
    - event: "PodEvicted"
      action: RestartJob
    - event: "TaskCompleted"
      action: RestartJob
    - event: "TaskFailed"
      action: RestartJob
    template: # Definition of the ps pod
      metadata:
        labels:
          role: leader
          kalavai.job.name: {{deployment_id}}
      spec:
        priorityClassName: "kalavai-system-priority"
        #serviceAccountName: gpustack-sa
        terminationGracePeriodSeconds: 60
        # nodeSelector:
        #   kalavai/role: server
        containers:
        - name: gpustack-leader
          image: docker.io/gpustack/gpustack:{{version}}
          args: 
          #- --config-file=/etc/config/server.yaml
          - --advertise-address=0.0.0.0
          - --huggingface-token=$HF_TOKEN 
          - --port=80
          - --token={{token}} 
          - --bootstrap-password={{admin_password}}
          - --cache-dir=/kalavai/cache 
          env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{deployment_id}}-secret
                key: hf_token
          - name: GPUSTACK_DB_POOL_SIZE
            value: "20"
          - name: GPUSTACK_DB_MAX_OVERFLOW
            value: "50"
          - name: GPUSTACK_DB_POOL_TIMEOUT
            value: "120"
          ports:
          - containerPort: 80
            name: model-port
          resources:
            requests:
              cpu: 1
              memory: 1Gi
              ephemeral-storage: {{working_memory}}Gi
            limits:
              cpu: 2
              memory: 2Gi
              ephemeral-storage: {{working_memory}}Gi
          volumeMounts:
          # - name: config-volume
          #   mountPath: /etc/config
          - name: cache
            mountPath: /kalavai/cache
        volumes:
        - name: cache
          emptyDir: {}
        # - name: config-volume
        #   configMap:
        #     # Reference the name of the ConfigMap defined above
        #     name: gpustack-config
        #     # Optional: You can set default file permissions (e.g., 0644)
        #     defaultMode: 0644
        restartPolicy: OnFailure
  - replicas: {{nvidia_gpus}}
    name: cuda-worker
    maxRetry: 100000
    policies:
    - event: "PodFailed"
      action: RestartTask
    - event: "PodEvicted"
      action: RestartTask
    - event: "TaskCompleted"
      action: RestartTask
    - event: "TaskFailed"
      action: RestartTask
    template: # Definition of worker pods
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        #serviceAccountName: gpustack-sa
        terminationGracePeriodSeconds: 60
        runtimeClassName: nvidia
        {% if NODE_SELECTORS %}
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
            {% if NODE_SELECTORS_OPS == "OR" %}
              {% for selector in NODE_SELECTORS %}
              - matchExpressions:
                - key: {{selector.name}}
                  operator: In
                  values:
                  {% for vals in selector.value %}
                  - "{{vals}}"
                  {% endfor %}
              {% endfor %}
            {% else %}
              - matchExpressions:
              {% for selector in NODE_SELECTORS %}
                {% for vals in selector.value %}
                - key: {{selector.name}}
                  operator: In
                  values:
                  - "{{vals}}"
                {% endfor %}
              {% endfor %}
            {% endif %}
        {% endif %}
        containers:
        - name: gpustack-worker
          image: docker.io/gpustack/gpustack:{{version}}
          command:
          - sh
          - -c
          - |
            ulimit -n 65535
            PS_HOST=`cat /etc/volcano/server.host`;
            echo $PS_HOST;
            NODE_PREFIX="cuda-worker-";
            NODE_NAME=$NODE_PREFIX$(($VC_TASK_INDEX+1));

            # --server-url http://gpustack-test-4f1e29-service.default.svc.cluster.local:80 \
            # --server-url http://$PS_HOST:80
            exec /usr/bin/entrypoint.sh \
              --server-url http://$PS_HOST:80 \
              --huggingface-token $HF_TOKEN \
              --token {{token}} \
              --worker-name $NODE_NAME
              --cache-dir /kalavai/cache \
              
            # fail if the connection was terminated
            exit 1
          env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{deployment_id}}-secret
                key: hf_token
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: 1
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: 1
              nvidia.com/gpumem-percentage: {{cuda_gpu_mem_percentage}}
              ephemeral-storage: {{working_memory}}Gi
          volumeMounts:
            - name: cache
              mountPath: /kalavai/cache
        volumes:
        - name: cache
          emptyDir: {}
        restartPolicy: OnFailure
  - replicas: {{amd_gpus}}
    name: rocm-worker
    maxRetry: 100000
    policies:
    - event: "PodFailed"
      action: RestartTask
    - event: "PodEvicted"
      action: RestartTask
    - event: "TaskCompleted"
      action: RestartTask
    - event: "TaskFailed"
      action: RestartTask
    template: # Definition of worker pods
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        #serviceAccountName: gpustack-sa
        terminationGracePeriodSeconds: 60
        {% if NODE_SELECTORS %}
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
            {% if NODE_SELECTORS_OPS == "OR" %}
              {% for selector in NODE_SELECTORS %}
              - matchExpressions:
                - key: {{selector.name}}
                  operator: In
                  values:
                  {% for vals in selector.value %}
                  - "{{vals}}"
                  {% endfor %}
              {% endfor %}
            {% else %}
              - matchExpressions:
              {% for selector in NODE_SELECTORS %}
                {% for vals in selector.value %}
                - key: {{selector.name}}
                  operator: In
                  values:
                  - "{{vals}}"
                {% endfor %}
              {% endfor %}
            {% endif %}
        {% endif %}
        containers:
        - name: gpustack-worker
          image: docker.io/gpustack/gpustack:{{version}}
          command:
          - sh
          - -c
          - |
            ulimit -n 65535
            PS_HOST=`cat /etc/volcano/server.host`;
            echo $PS_HOST;
            NODE_PREFIX="rocm-worker-";
            NODE_NAME=$NODE_PREFIX$(($VC_TASK_INDEX+1));

            exec /usr/bin/entrypoint.sh \
              --server-url http://$PS_HOST:8080 \
              --cache-dir /kalavai/cache \
              --huggingface-token $HF_TOKEN \
              --token {{token}} \
              --worker-name $NODE_NAME
            # fail if the connection was terminated
            exit 1
          env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{deployment_id}}-secret
                key: hf_token
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              amd.com/gpu: 1
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              amd.com/gpu: 1
              ephemeral-storage: {{working_memory}}Gi
        #   volumeMounts:
        #     - name: cache
        #       mountPath: /kalavai/cache
        # volumes:
        # - name: cache
        #   emptyDir: {}
        restartPolicy: OnFailure      
---
# deploy NodePort to expose model
apiVersion: v1
kind: Service
metadata:
  name: {{deployment_id}}-service
  labels:
    # must have this!
    kalavai.job.name: {{deployment_id}}
spec:
  type: NodePort
  selector:
    role: leader
    kalavai.job.name: {{deployment_id}}
  ports:
    - name: ui
      port: 80
      targetPort: 80
      protocol: TCP
    {% if nodeport != "" %}
      nodePort: {{nodeport}}
    {% endif %}
