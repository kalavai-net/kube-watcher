apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_id}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_id}}
spec:
  schedulerName: volcano
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  policies: 
  - event: PodEvicted
    action: RestartJob
  - event: PodFailed
    action: RestartJob
  - event: TaskCompleted
    action: RestartJob
  - event: Unknown
    action: RestartJob
  tasks:
  - replicas: 1   # One ps pod specified
    name: worker
    template: # Definition of the ps pod
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
          role: leader
      spec:
        runtimeClassName: nvidia
        containers:
        - name: container
        {% if gpus > 0 %}
          image: ghcr.io/speaches-ai/speaches:latest-cuda-12.4.1
        {% else %}
          image: ghcr.io/speaches-ai/speaches:latest-cpu
        {% endif %}
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - |
            export KOKORO_REVISION=c97b7bbc3e60f447383c79b2f94fee861ff156ac
            # Download the ONNX model (~346 MBs)
            huggingface-cli download hexgrad/Kokoro-82M --include 'kokoro-v0_19.onnx' --revision $KOKORO_REVISION
            # Download the voices.bin (~5.5 MBs) file
            curl --location --output /home/ubuntu/.cache/huggingface/hub/models--hexgrad--Kokoro-82M/snapshots/$KOKORO_REVISION/voices.bin https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/voices.bin
            # Download all English voices (~4.5 minutes)
            huggingface-cli download rhasspy/piper-voices --include 'en/**/*' 'voices.json'
            # run server
            uvicorn --factory --host 0.0.0.0 --port 8000 speaches.main:create_app
          ports:
          - containerPort: 8000
            name: container-port
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
            {% if gpus and gpus > 0 %}
              nvidia.com/gpu: {{gpus}}
            {% endif %}
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
            {% if gpus > 0 %}
              nvidia.com/gpu: {{gpus}}
            {% endif %}
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 10
        restartPolicy: OnFailure

