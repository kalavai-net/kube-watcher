# vLLM template

Version v0.9.1.

## When to use vLLM as a model engine

vLLM is ideal to load large language models (text only, multimodal, embedding) across one or multiple machines. It is specifically designed to work on GPUs only. Support for quantized models is limited to AWQ and GPTQ.

vLLM is the tool of choice when one has enough GPUs available (with enough vRAM) to load the model weights, with non quantized models, or with AWQ or GPTQ quantized models.


## Engine arguments

A list and explanation of every engine argument for vLLM can be found: https://docs.vllm.ai/en/v0.9.1/models/engine_args.html

## Supported models

Here is a full list of supported model architectures:

### Text-only Language models
AquilaForCausalLM
ArcticForCausalLM
BaiChuanForCausalLM
BambaForCausalLM
BloomForCausalLM
BartForConditionalGeneration
ChatGLMModel, ChatGLMForConditionalGeneration
CohereForCausalLM, Cohere2ForCausalLM
DbrxForCausalLM
DeciLMForCausalLM
DeepseekForCausalLM
DeepseekV2ForCausalLM
DeepseekV3ForCausalLM
ExaoneForCausalLM
FalconForCausalLM
FalconMambaForCausalLM
FalconH1ForCausalLM
GemmaForCausalLM
Gemma2ForCausalLM
Gemma3ForCausalLM
GlmForCausalLM
Glm4ForCausalLM
GPT2LMHeadModel
GPTBigCodeForCausalLM
GPTJForCausalLM
GPTNeoXForCausalLM
GraniteForCausalLM
GraniteMoeForCausalLM
GraniteMoeHybridForCausalLM
GraniteMoeSharedForCausalLM
GritLM
Grok1ModelForCausalLM
InternLMForCausalLM
InternLM2ForCausalLM
InternLM3ForCausalLM
JAISLMHeadModel
JambaForCausalLM
LlamaForCausalLM
MambaForCausalLM
MiniCPMForCausalLM
MiniCPM3ForCausalLM
MistralForCausalLM
MixtralForCausalLM
MPTForCausalLM
NemotronForCausalLM
NemotronHForCausalLM
OLMoForCausalLM
OLMo2ForCausalLM
OLMoEForCausalLM
OPTForCausalLM
OrionForCausalLM
PhiForCausalLM
Phi3ForCausalLM
Phi3SmallForCausalLM
PhiMoEForCausalLM
PersimmonForCausalLM
Plamo2ForCausalLM
QWenLMHeadModel
Qwen2ForCausalLM
Qwen2MoeForCausalLM
Qwen3ForCausalLM
Qwen3MoeForCausalLM
StableLmForCausalLM
Starcoder2ForCausalLM
SolarForCausalLM
TeleChat2ForCausalLM
TeleFLMForCausalLM
XverseForCausalLM
MiniMaxText01ForCausalLM
Zamba2ForCausalLM

### Pooling (embedding) models

BertModel
Gemma2Model
GritLM
GteModel
GteNewModel
ModernBertModel
NomicBertModel
LlamaModel, LlamaForCausalLM, MistralModel, etc.
Qwen2Model, Qwen2ForCausalLM
RobertaModel, RobertaForMaskedLM
LlavaNextForConditionalGeneration
Phi3VForCausalLM


### Reward models

InternLM2ForRewardModel
LlamaForCausalLM
Qwen2ForRewardModel

### Classification

JambaForSequenceClassification

### Sentence pair scoring

BertForSequenceClassification
RobertaForSequenceClassification
XLMRobertaForSequenceClassification


### Multimodal languages

AriaForConditionalGeneration
AyaVisionForConditionalGeneration
Blip2ForConditionalGeneration
ChameleonForConditionalGeneration
DeepseekVLV2ForCausalLM^
Florence2ForConditionalGeneration
FuyuForCausalLM
Gemma3ForConditionalGeneration
GLM4VForCausalLM^
GraniteSpeechForConditionalGeneration
H2OVLChatModel
Idefics3ForConditionalGeneration
InternVLChatModel
KimiVLForConditionalGeneration
Llama4ForConditionalGeneration
LlavaForConditionalGeneration
LlavaNextForConditionalGeneration
LlavaNextVideoForConditionalGeneration
LlavaOnevisionForConditionalGeneration
MiniCPMO
MiniCPMV
MiniMaxVL01ForConditionalGeneration
Mistral3ForConditionalGeneration
MllamaForConditionalGeneration
MolmoForCausalLM
NVLM_D_Model
Ovis
PaliGemmaForConditionalGeneration
Phi3VForCausalLM
Phi4MMForCausalLM
PixtralForConditionalGeneration
QwenVLForConditionalGeneration^
Qwen2AudioForConditionalGeneration
Qwen2VLForConditionalGeneration
Qwen2_5_VLForConditionalGeneration
Qwen2_5OmniThinkerForConditionalGeneration
SkyworkR1VChatModel
SmolVLMForConditionalGeneration
TarsierForConditionalGeneration
