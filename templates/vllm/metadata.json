{
  "name": "vLLM",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "docs": "https://docs.vllm.ai/",
  "icon": "https://docs.vllm.ai/en/latest/assets/logos/vllm-logo-only-light.ico",
  "template_rules": "vLLM is an LLM model serving template on GPUs. With the vLLM template you can deploy a vLLM server to serve your LLM models in your cluster. vLLM is designed to work with GPU nodes only. It supports the following quantizations: AWQ and GPTQ. Does not support GGUF models.\nSupported models: \n\nText-only Language models: Aquila, Aquila2d, Arcticd, Baichuan2, Baichuand, Bambad, BLOOM, BLOOMZ, BLOOMChatd, BARTd, ChatGLMd, Command-Rd, DBRXd, DeciLMd, DeepSeekd, DeepSeek-V2d, DeepSeek-V3d, dots.llm1d, Ernie4.5d, Ernie4.5MoEd, EXAONE-3d, Falcond, FalconMambad, Falcon-H1d, Gemmad, Gemma 2d, Gemma 3d, Gemma 3nd, GLM-4d, GLM-4-0414d, GPT-2d, StarCoder, SantaCoder, WizardCoderd, GPT-Jd, GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLMd, Granite 3.0, Granite 3.1, PowerLMd, Granite 3.0 MoE, PowerMoEd, Granite 4.0 MoE Hybridd, Granite MoE Sharedd, GritLMd, Grok1d, Hunyuan-80B-A13Bd, InternLMd, InternLM2d, InternLM3d, Jaisd, Jambad, Llama 3.1, Llama 3, Llama 2, LLaMA, Yid, Mambad, Mamba2d, MiniCPMd, MiniCPM3d, Mistral, Mistral-Instructd, Mixtral-8x7B, Mixtral-8x7B-Instructd, MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriterd, Nemotron-3, Nemotron-4, Minitrond, Nemotron-Hd, OLMod, OLMo2d, OLMoEd, OPT, OPT-IMLd, Oriond, Phid, Phi-4, Phi-3d, Phi-3-Smalld, Phi-3.5-MoEd, Persimmond, PLaMo2d, Qwend, QwQ, Qwen2d, Qwen2MoEd, Qwen3d, Qwen3MoEd, StableLMd, Starcoder2d, Solar Prod, TeleChat2d, TeleFLMd, XVERSEd, MiniMax-Textd, MiniMax-Textd, Zamba2.\n\nPooling models: BERT-based, Gemma 2-based, GritLM, Arctic-Embed-2.0-M, mGTE-TRM (see note), ModernBERT-based, Nomic BERT, Llama-based, Qwen2-based, Qwen3-based, RoBERTa-based.\n\nReward models: InternLM2-based, Llama-based, Qwen2-based, Qwen2-based.\n\nClassification: Jamba, GPT2.\n\nSentence pair scoring: BERT-based, Qwen2-based, Qwen3-based, RoBERTa-based, XLM-RoBERTa-based.\n\nMultimodal: Aria, Aya Vision, BLIP-2, Chameleon, DeepSeek-VL2, Florence-2, Fuyu, Gemma 3, GLM-4V, GLM-4.1V-Thinking, Granite Speech, H2OVL, Idefics3, InternVL 3.0, InternVideo 2.5, InternVL 2.5, Mono-InternVL, InternVL 2.0, Keye-VL-8B-Preview, Kimi-VL-A3B-Instruct, Kimi-VL-A3B-Thinking, Llama 4, LLaVA-1.5, LLaVA-NeXT, LLaVA-NeXT-Video, LLaVA-Onevision, MiniCPM-O, MiniCPM-V, MiniMax-VL, Mistral3, Llama 3.2, Molmo, NVLM-D 1.0, Ovis2, Ovis1.6, PaliGemma, PaliGemma 2, Phi-3-Vision, Phi-3.5-Vision, Phi-4-multimodal, Pixtral, Qwen-VL, Qwen2-Audio, QVQ, Qwen2-VL, Qwen2.5-VL, Qwen2.5-Omni, Skywork-R1V-38B, SmolVLM2, Tarsier, Tarsier2.\n\nTranscription: Whisper.\n\n",
  "values_rules": "'model_id' corresponds to the huggingface model id.\n||\n'cpus' refers to the number of cpus to use per worker, and its value should always be less than the number of freely available CPUs in each node on the pool.\n||\n'gpus' refers to the number of GPUs to use on any single node used in the deployment; generally this value should be 1 unless at least as many nodes as 'workers' have multiple GPUs.\n||\n'working_memory' sets the disk memory set aside for each worker node, and should be enough to accommodate the size of the model; a good heuristic is to leave 2 times the model size (in billions of parameters), in GBs (e.g. if the model is 3B parameters, set the working memory to 6 GBs)\n||\n'workers' sets the number of nodes or devices to use in the deployment, and should always be less or equal than the number of nodes with GPUs."
}
