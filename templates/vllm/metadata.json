{
  "name": "vLLM",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "docs": "https://docs.vllm.ai/",
  "icon": "https://docs.vllm.ai/en/latest/assets/logos/vllm-logo-only-light.ico",
  "info": "With the vLLM template you can deploy a vLLM server to serve your LLM models in your cluster. Supported models: \n\nText-only Language models: AquilaForCausalLM, ArcticForCausalLM, BaiChuanForCausalLM, BambaForCausalLM, BloomForCausalLM, BartForConditionalGeneration, ChatGLMModel, ChatGLMForConditionalGeneration, CohereForCausalLM, Cohere2ForCausalLM, DbrxForCausalLM, DeciLMForCausalLM, DeepseekForCausalLM, DeepseekV2ForCausalLM, DeepseekV3ForCausalLM, ExaoneForCausalLM, FalconForCausalLM, FalconMambaForCausalLM, FalconH1ForCausalLM, GemmaForCausalLM, Gemma2ForCausalLM, Gemma3ForCausalLM, GlmForCausalLM, Glm4ForCausalLM, GPT2LMHeadModel, GPTBigCodeForCausalLM, GPTJForCausalLM, GPTNeoXForCausalLM, GraniteForCausalLM, GraniteMoeForCausalLM, GraniteMoeHybridForCausalLM, GraniteMoeSharedForCausalLM, GritLM, Grok1ModelForCausalLM, InternLMForCausalLM, InternLM2ForCausalLM, InternLM3ForCausalLM, JAISLMHeadModel, JambaForCausalLM, LlamaForCausalLM, MambaForCausalLM, MiniCPMForCausalLM, MiniCPM3ForCausalLM, MistralForCausalLM, MixtralForCausalLM, MPTForCausalLM, NemotronForCausalLM, NemotronHForCausalLM, OLMoForCausalLM, OLMo2ForCausalLM, OLMoEForCausalLM, OPTForCausalLM, OrionForCausalLM, PhiForCausalLM, Phi3ForCausalLM, Phi3SmallForCausalLM, PhiMoEForCausalLM, PersimmonForCausalLM, Plamo2ForCausalLM, QWenLMHeadModel, Qwen2ForCausalLM, Qwen2MoeForCausalLM, Qwen3ForCausalLM, Qwen3MoeForCausalLM, StableLmForCausalLM, Starcoder2ForCausalLM, SolarForCausalLM, TeleChat2ForCausalLM, TeleFLMForCausalLM, XverseForCausalLM, MiniMaxText01ForCausalLM, Zamba2ForCausalLM; Pooling (embedding) models: BertModel, Gemma2Model, GritLM, GteModel, GteNewModel, ModernBertModel, NomicBertModel, LlamaModel, LlamaForCausalLM, MistralModel, Qwen2Model, Qwen2ForCausalLM, RobertaModel, RobertaForMaskedLM, LlavaNextForConditionalGeneration, Phi3VForCausalLM; Reward models: InternLM2ForRewardModel, LlamaForCausalLM, Qwen2ForRewardModel; Classification: JambaForSequenceClassification; Sentence pair scoring: BertForSequenceClassification, RobertaForSequenceClassification, XLMRobertaForSequenceClassification; Multimodal languages: AriaForConditionalGeneration, AyaVisionForConditionalGeneration, Blip2ForConditionalGeneration, ChameleonForConditionalGeneration, DeepseekVLV2ForCausalLM, Florence2ForConditionalGeneration, FuyuForCausalLM, Gemma3ForConditionalGeneration, GLM4VForCausalLM, GraniteSpeechForConditionalGeneration, H2OVLChatModel, Idefics3ForConditionalGeneration, InternVLChatModel, KimiVLForConditionalGeneration, Llama4ForConditionalGeneration, LlavaForConditionalGeneration, LlavaNextForConditionalGeneration, LlavaNextVideoForConditionalGeneration, LlavaOnevisionForConditionalGeneration, MiniCPMO, MiniCPMV, MiniMaxVL01ForConditionalGeneration, Mistral3ForConditionalGeneration, MllamaForConditionalGeneration, MolmoForCausalLM, NVLM_D_Model, Ovis, PaliGemmaForConditionalGeneration, Phi3VForCausalLM, Phi4MMForCausalLM, PixtralForConditionalGeneration, QwenVLForConditionalGeneration, Qwen2AudioForConditionalGeneration, Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, Qwen2_5OmniThinkerForConditionalGeneration, SkyworkR1VChatModel, SmolVLMForConditionalGeneration, TarsierForConditionalGeneration"
}
