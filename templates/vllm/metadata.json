{
  "name": "vLLM",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "docs": "https://docs.vllm.ai/",
  "icon": "https://docs.vllm.ai/en/latest/assets/logos/vllm-logo-only-light.ico",
  "template_rules": "vLLM is an LLM model serving template on GPUs. With the vLLM template you can deploy a vLLM server to serve your LLM models in your cluster. vLLM is designed to work with GPU nodes.\nSupported models: \n\nText-only Language models: AquilaForCausalLM, ArcticForCausalLM, BaiChuanForCausalLM, BambaForCausalLM, BloomForCausalLM, BartForConditionalGeneration, ChatGLMModel, ChatGLMForConditionalGeneration, CohereForCausalLM, Cohere2ForCausalLM, DbrxForCausalLM, DeciLMForCausalLM, DeepseekForCausalLM, DeepseekV2ForCausalLM, DeepseekV3ForCausalLM, ExaoneForCausalLM, FalconForCausalLM, FalconMambaForCausalLM, FalconH1ForCausalLM, GemmaForCausalLM, Gemma2ForCausalLM, Gemma3ForCausalLM, GlmForCausalLM, Glm4ForCausalLM, GPT2LMHeadModel, GPTBigCodeForCausalLM, GPTJForCausalLM, GPTNeoXForCausalLM, GraniteForCausalLM, GraniteMoeForCausalLM, GraniteMoeHybridForCausalLM, GraniteMoeSharedForCausalLM, GritLM, Grok1ModelForCausalLM, InternLMForCausalLM, InternLM2ForCausalLM, InternLM3ForCausalLM, JAISLMHeadModel, JambaForCausalLM, LlamaForCausalLM, MambaForCausalLM, MiniCPMForCausalLM, MiniCPM3ForCausalLM, MistralForCausalLM, MixtralForCausalLM, MPTForCausalLM, NemotronForCausalLM, NemotronHForCausalLM, OLMoForCausalLM, OLMo2ForCausalLM, OLMoEForCausalLM, OPTForCausalLM, OrionForCausalLM, PhiForCausalLM, Phi3ForCausalLM, Phi3SmallForCausalLM, PhiMoEForCausalLM, PersimmonForCausalLM, Plamo2ForCausalLM, QWenLMHeadModel, Qwen2ForCausalLM, Qwen2MoeForCausalLM, Qwen3ForCausalLM, Qwen3MoeForCausalLM, StableLmForCausalLM, Starcoder2ForCausalLM, SolarForCausalLM, TeleChat2ForCausalLM, TeleFLMForCausalLM, XverseForCausalLM, MiniMaxText01ForCausalLM, Zamba2ForCausalLM; Pooling (embedding) models: BertModel, Gemma2Model, GritLM, GteModel, GteNewModel, ModernBertModel, NomicBertModel, LlamaModel, LlamaForCausalLM, MistralModel, Qwen2Model, Qwen2ForCausalLM, RobertaModel, RobertaForMaskedLM, LlavaNextForConditionalGeneration, Phi3VForCausalLM; Reward models: InternLM2ForRewardModel, LlamaForCausalLM, Qwen2ForRewardModel; Classification: JambaForSequenceClassification; Sentence pair scoring: BertForSequenceClassification, RobertaForSequenceClassification, XLMRobertaForSequenceClassification; Multimodal languages: AriaForConditionalGeneration, AyaVisionForConditionalGeneration, Blip2ForConditionalGeneration, ChameleonForConditionalGeneration, DeepseekVLV2ForCausalLM, Florence2ForConditionalGeneration, FuyuForCausalLM, Gemma3ForConditionalGeneration, GLM4VForCausalLM, GraniteSpeechForConditionalGeneration, H2OVLChatModel, Idefics3ForConditionalGeneration, InternVLChatModel, KimiVLForConditionalGeneration, Llama4ForConditionalGeneration, LlavaForConditionalGeneration, LlavaNextForConditionalGeneration, LlavaNextVideoForConditionalGeneration, LlavaOnevisionForConditionalGeneration, MiniCPMO, MiniCPMV, MiniMaxVL01ForConditionalGeneration, Mistral3ForConditionalGeneration, MllamaForConditionalGeneration, MolmoForCausalLM, NVLM_D_Model, Ovis, PaliGemmaForConditionalGeneration, Phi3VForCausalLM, Phi4MMForCausalLM, PixtralForConditionalGeneration, QwenVLForConditionalGeneration, Qwen2AudioForConditionalGeneration, Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, Qwen2_5OmniThinkerForConditionalGeneration, SkyworkR1VChatModel, SmolVLMForConditionalGeneration, TarsierForConditionalGeneration",
  "values_rules": "'model_id' corresponds to the huggingface model id.\n||\n'cpus' refers to the number of cpus to use per worker, and its value should always be less than the number of freely available CPUs in each node on the pool.\n||\n'gpus' refers to the number of GPUs to use on any single node used in the deployment; generally this value should be 1 unless at least as many nodes as 'workers' have multiple GPUs.\n||\n'working_memory' sets the disk memory set aside for each worker node, and should be enough to accommodate the size of the model; a good heuristic is to leave 2 times the model size (in billions of parameters), in GBs (e.g. if the model is 3B parameters, set the working memory to 6 GBs)\n||\n'workers' sets the number of nodes or devices to use in the deployment, and should always be less or equal than the number of nodes with GPUs."
}
