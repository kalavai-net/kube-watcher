{
  "name": "vLLM",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "docs": "https://docs.vllm.ai/",
  "icon": "https://docs.vllm.ai/en/latest/assets/logos/vllm-logo-only-light.ico",
  "info": ""
}