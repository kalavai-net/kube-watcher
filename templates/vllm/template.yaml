apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: {{deployment_id}}
  labels:
    # must have this label
    kalavai.job.name: {{deployment_id}}
spec:
  queue: default
  schedulerName: volcano
  plugins:
    env: []
    svc: ["--disable-network-policy=true"]
  policies: 
  - event: PodEvicted # Restart the job when a pod is evicted.
    action: RestartJob
  - event: TaskCompleted
    action: CompleteJob
  tasks:
  - replicas: 1   # One ps pod specified
    name: registrar
    template: # Definition of the ps pod
      terminationGracePeriodSeconds: 30 #give enough time to the preStop hook
      metadata:
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        containers:
        - name: vllm-leader
          image: docker.io/bundenth/kalavai-utils:latest
          env:
          - name: VC_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          command:
          - sh
          - -c
          - |
            if [ ! -z "{{litellm_key}}" ]; then
              echo $VC_NAMESPACE;
              # wait for model to be served
              echo "Waiting for model service..."
              PS_HOST=`cat /etc/volcano/server.host`;
              /workspace/wait_for_service.sh --servers="$PS_HOST" --port=8080
              # Register model with LiteLLM
              PS_HOST=`cat /etc/volcano/server.host`;
              LITELLM_MODEL_NAME="{{deployment_id}}";
              echo "Creating new entry on LiteLLM: (host: $PS_HOST) - (model id: $LITELLM_MODEL_NAME)";
              API_BASE="http://"$PS_HOST"."$VC_NAMESPACE":8080/v1";
              /workspace/register_model.sh \
                --litellm_base_url={{litellm_base_url}} \
                --litellm_key={{litellm_key}} \
                --litellm_model_name="$LITELLM_MODEL_NAME" \
                --model_id={{model_id}} \
                --provider=hosted_vllm \
                --api_base=$API_BASE \
                --model_info='{"extra": "{{extra}}", "cpus": "{{cpus}}", "gpus": "{{gpus}}", "memory": "{{memory}}", "tensor_parallel_size": "{{tensor_parallel_size}}", "pipeline_parallel_size": "{{pipeline_parallel_size}}"}'
            else
              echo "[Warning] Missing LiteLLM base url. Model not publicly registered"
              tail -f /dev/null
            fi
          lifecycle:
            preStop:
              exec:
                command: 
                - sh
                - -c
                - |       
                  if [ ! -z "{{litellm_key}}" ]; then
                    LITELLM_MODEL_NAME="{{deployment_id}}";         
                    MODEL_ID=$(python3 /workspace/get_litellm_id.py \
                      --litellm_url={{litellm_base_url}} \
                      --api_key={{litellm_key}} \
                      --model_name="$LITELLM_MODEL_NAME");
                    curl -X POST "{{litellm_base_url}}/model/delete" \
                        -H 'Authorization: Bearer {{litellm_key}}' \
                        -H "accept: application/json" \
                        -H "Content-Type: application/json" \
                        -d '{ "id": "'$MODEL_ID'"}';
                  fi
          resources:
            requests:
              cpu: 0.1
              memory: 0.1Gi
            limits:
              cpu: 0.1
              memory: 0.1Gi
        restartPolicy: OnFailure
  - replicas: 1   # One ps pod specified
    name: server
    template: # Definition of the ps pod
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          role: leader
          kalavai.job.name: {{deployment_id}}
      spec:
        runtimeClassName: nvidia
        containers:
        - name: vllm-leader
          image: docker.io/bundenth/ray-vllm:latest
          command:
          - sh
          - -c
          - |
            # Download model weights
            /home/ray/workspace/download_model.sh \
              --model_id={{model_id}} \
              --remote_dir="/home/ray/cache";
            RAY_BACKEND_LOG_LEVEL=error /home/ray/workspace/ray_init.sh leader --ray_cluster_size=$(({{workers}})) --ray_object_store_memory={{memory * 500000000}};
            sleep 30;
            nvidia-smi;
            ray status;
            # Run model
            /home/ray/workspace/run_model.sh \
              --model_path="/home/ray/cache/{{model_id}}" \
              --model_id={{model_id}} \
              --extra='{{extra}}' \
              --tensor_parallel_size={{tensor_parallel_size}} \
              --pipeline_parallel_size={{pipeline_parallel_size}} \
              --lora_modules="{{lora_modules}}";
            exit 1
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          ports:
          - containerPort: 8080
            name: model-port
          - containerPort: 8265
            name: dashboard-port
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: {{working_memory}}Gi
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: {{working_memory}}Gi
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - name: cache
              mountPath: /home/ray/cache
        volumes:
        - name: cache
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{memory * 0.5}}Gi
        restartPolicy: OnFailure
  - replicas: {{workers - 1}}
    name: worker
    policies:
    - event: TaskCompleted  # The job will be marked as completed when two worker pods finish tasks.
      action: CompleteJob
    template: # Definition of worker pods
      metadata:
        annotations:
          # must have these annotations
          {{nouse_gputype}}
          {{use_gputype}}
        labels:
          kalavai.job.name: {{deployment_id}}
      spec:
        runtimeClassName: nvidia
        containers:
        - name: vllm-worker
          image: docker.io/bundenth/ray-vllm:latest #v1.1.4
          command:
          - sh
          - -c
          - |
            PS_HOST=`cat /etc/volcano/server.host`;
            # Download model weights
            /home/ray/workspace/download_model.sh \
              --model_id={{model_id}} \
              --remote_dir="/home/ray/cache";
            nvidia-smi;
            RAY_BACKEND_LOG_LEVEL=error /home/ray/workspace/ray_init.sh worker --ray_address=$PS_HOST --ray_port=6379 --ray_object_store_memory={{memory * 500000000}} --ray_block=1
          env:
          - name: HF_TOKEN
            value: {{hf_token}}
          resources:
            requests:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: {{working_memory}}Gi
            limits:
              cpu: {{cpus}}
              memory: {{memory}}Gi
              nvidia.com/gpu: {{gpus}}
              ephemeral-storage: {{working_memory}}Gi
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - name: cache
              mountPath: /home/ray/cache
        volumes:
        - name: cache
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{memory * 0.5}}Gi
        restartPolicy: OnFailure
